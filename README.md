# ANLI Round 2 â€“ Natural Language Inference (NLI) Project
End-to-End ML + Transformer Fine-Tuning + Docker Deployment

Comprehensive exploration and production-ready implementation of Natural Language Inference models on the Adversarial NLI (ANLI) Round 2 dataset â€” covering everything from EDA â†’ ML Baselines â†’ Transformer Fine-Tuning â†’ Deployment with Docker.

ğŸ¯ Project Overview

Task: Natural Language Inference (NLI)
Dataset: Adversarial NLI (ANLI) Round 2
Goal: Predict the relationship between premise and hypothesis:

Entailment

Neutral

Contradiction

Dataset Size (ANLI R2):

Split	Count
Train	45,460
Dev	1,000
Test	1,000

ANLI is intentionally adversarial and significantly harder than SNLI/MNLI, making it a strong benchmark for model robustness.

ğŸ“Š Results Summary
Model	Accuracy	Macro F1	Notes
DistilRoBERTa (fine-tuned)	XX%	XX	Best model; fine-tuned for NLI
XGBoost	~38%	~0.33	Strongest ML baseline
Linear SVM	~36%	~0.33	Good baseline
Logistic Regression	~35%	~0.33	Baseline
DistilRoBERTa (pretrained, no FT)	~33%	~0.24	Zero-shot baseline

(Replace XX% with your actual results)

ğŸ§  High-Level Workflow

âœ” Exploratory Data Analysis
âœ” Preprocessing & text construction
âœ” Traditional ML baselines
âœ” Transformer fine-tuning
âœ” Evaluation on Dev & Test
âœ” Docker-based inference pipeline

ğŸ“ Repository Structure
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb                     # Data exploration
â”‚   â”œâ”€â”€ baseline_ml.ipynb             # LogReg, SVM, XGBoost
â”‚   â””â”€â”€ RoBerta.ipynb                 # Transformer fine-tuning
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loading.py               # Load ANLI R2 dataset
â”‚   â”œâ”€â”€ preprocessing.py              # Tokenization + text prep
â”‚   â”œâ”€â”€ evaluation.py                 # Metrics & reports
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py             # Train TF-IDF + ML models
â”‚   â”œâ”€â”€ train_transformer.py          # Train DistilRoBERTa
â”‚   â””â”€â”€ inference.py                  # Run inference on text pairs
â”‚
â”œâ”€â”€ models/                           # (Ignored in Git; local only)
â”‚   â””â”€â”€ roberta_anli_r2/              # Saved fine-tuned model
â”‚
â”œâ”€â”€ run_pipeline.py                   # Unified CLI pipeline
â”œâ”€â”€ Dockerfile                        # Inference container
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .dockerignore

ğŸš€ Two Ways to Use This Project
1ï¸âƒ£ Jupyter Notebooks (Exploration)

Best for experimentation and understanding the pipeline.

ğŸ“Œ Includes:

EDA.ipynb â†’ Label dist, text length, examples, imbalance

baseline_ml.ipynb â†’ TF-IDF + LogReg/SVM/XGBoost

RoBerta.ipynb â†’ End-to-end fine-tuning with HuggingFace

Great for learning and showcasing methodology.

2ï¸âƒ£ Production Pipeline (Automation + Reproducibility)

Everything modular, script-based, and deployable.

ğŸŒŸ Features:

Run classical ML baselines:

python run_pipeline.py --mode eval_baseline


Run transformer inference:

python run_pipeline.py --mode demo \
    --premise "A man is playing music" \
    --hypothesis "A man is playing guitar"


Train models via scripts:

python scripts/train_baseline.py
python scripts/train_transformer.py

ğŸ“¦ Docker Deployment (Inference-Ready Container)

This project includes a Dockerfile that packages:

The inference script

All dependencies

Model loading

A default demo prediction

Build the image:
docker build -t anli-nli .

Run inference:
docker run --rm anli-nli


(Default CMD runs a demo premiseâ€“hypothesis pair)

Example CMD inside Dockerfile:
CMD ["python", "run_pipeline.py",
     "--mode", "demo",
     "--premise", "A man is playing music",
     "--hypothesis", "A man is playing guitar"]


Even if the interviewer doesn't run it,
having Docker in the repo shows production-readiness.

ğŸ” Learning Path (Recommended)
1. Explore the dataset
notebooks/eda.ipynb

2. Build ML baselines
notebooks/baseline_ml.ipynb
scripts/train_baseline.py

3. Train Transformer
notebooks/RoBerta.ipynb
scripts/train_transformer.py

4. Evaluate & compare
src/evaluation.py

5. Deploy model with Docker
Dockerfile
run_pipeline.py
scripts/inference.py

ğŸ§ª Technologies Used
ML / DL

PyTorch

HuggingFace Transformers

scikit-learn

XGBoost

Data

Datasets (HuggingFace)

pandas, NumPy

Deployment

Docker

CLI pipeline

Development

Jupyter

Python 3.11

ğŸ“ˆ Performance Comparison (Baseline â†’ Best)
DistilRoBERTa baseline            33%   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
Logistic Regression               35%   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
Linear SVM                        36%   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
XGBoost                           38%   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
DistilRoBERTa Fine-Tuned          XX%   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â­ Best


(Replace XX% with your actual final model accuracy)

ğŸ¯ Project Goals Achieved

âœ” Comprehensive EDA
âœ” Implementation of traditional ML baselines
âœ” Transformer fine-tuning with HuggingFace
âœ” Clean modular codebase
âœ” Evaluation on dev & test
âœ” Docker-based deployment
âœ” Production-ready pipeline
âœ” Well-structured repository
âœ” Reproducible results

ğŸ“¬ Final Notes

This project demonstrates:

Strong understanding of NLP, NLI, and transformers

Ability to structure modular ML pipelines

Clear documentation and reproducible experiments

Deployment mindset (Docker + inference pipeline)

Perfect for interviews, portfolio, and demonstrating real ML engineering skill.
