# ANLI Round 2 â€“ Natural Language Inference (NLI) Project  
## End-to-End ML + Transformer Fine-Tuning + Docker Deployment

Comprehensive exploration and production-ready implementation of **Natural Language Inference** models on the **Adversarial NLI (ANLI) Round 2 dataset** â€” covering everything from **EDA** â†’ **ML Baselines** â†’ **Transformer Fine-Tuning** â†’ **Deployment with Docker**.

---

## ğŸ¯ Project Overview

**Task:** Natural Language Inference (NLI)  
**Dataset:** Adversarial NLI (ANLI) Round 2  
**Goal:** Predict the relationship between *premise* and *hypothesis*:

- Entailment  
- Neutral  
- Contradiction  

**Dataset Size (ANLI R2):**

| Split | Count |
|-------|--------|
| Train | 45,460 |
| Dev   | 1,000  |
| Test  | 1,000  |

ANLI is **intentionally adversarial** and significantly harder than SNLI/MNLI, making it a strong benchmark for model robustness.

---

## ğŸ“Š Results Summary

| Model                         | Accuracy | Macro F1 | Notes                          |
|------------------------------|----------|----------|--------------------------------|
| **DistilRoBERTa (fine-tuned)** | XX%      | XX       | Best model; fine-tuned for NLI |
| XGBoost                       | ~38%     | ~0.33    | Strongest ML baseline          |
| Linear SVM                    | ~36%     | ~0.33    | Good baseline                  |
| Logistic Regression           | ~35%     | ~0.33    | Baseline                       |
| DistilRoBERTa (no fine-tune) | ~33%     | ~0.24    | Zero-shot baseline             |

> Replace **XX%** with your actual final results.

---

## ğŸ§  High-Level Workflow

âœ” Exploratory Data Analysis  
âœ” Preprocessing & feature construction  
âœ” Traditional ML baselines  
âœ” Transformer fine-tuning (DistilRoBERTa)  
âœ” Evaluation on Dev & Test  
âœ” Docker-based inference pipeline  

---

## ğŸ“ Repository Structure

```plaintext
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb                 # Data exploration
â”‚   â”œâ”€â”€ baseline_ml.ipynb         # LogReg, SVM, XGBoost
â”‚   â””â”€â”€ RoBerta.ipynb             # Transformer fine-tuning
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loading.py           # Load ANLI R2 dataset
â”‚   â”œâ”€â”€ preprocessing.py          # Text construction & tokenization
â”‚   â”œâ”€â”€ evaluation.py             # Metrics & reports
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py         # Train TF-IDF + ML models
â”‚   â”œâ”€â”€ train_transformer.py      # Train DistilRoBERTa
â”‚   â””â”€â”€ inference.py              # Run inference on text pairs
â”‚
â”œâ”€â”€ models/                       # (Ignored in Git; local only)
â”‚   â””â”€â”€ roberta_anli_r2/          # Saved fine-tuned transformer model
â”‚
â”œâ”€â”€ run_pipeline.py               # Unified CLI pipeline
â”œâ”€â”€ Dockerfile                    # Inference container
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .dockerignore




ğŸš€ Two Ways to Use This Project
1ï¸âƒ£ Jupyter Notebooks (Exploration)

Best for experimentation and understanding the pipeline.

Includes:

notebooks/eda.ipynb â†’ label distribution, text length, examples

notebooks/baseline_ml.ipynb â†’ TF-IDF + Logistic/SVM/XGBoost

notebooks/RoBerta.ipynb â†’ full DistilRoBERTa fine-tuning

Great for learning and showcasing methodology.

2ï¸âƒ£ Production Pipeline (Scripts + Automation)

Everything modular, script-based, and deployable.

Run classical ML baselines:
python run_pipeline.py --mode eval_baseline

Run transformer inference:
python run_pipeline.py --mode demo \
    --premise "A man is playing music" \
    --hypothesis "A man is playing guitar"

Train models manually:
python scripts/train_baseline.py
python scripts/train_transformer.py

ğŸ“¦ Docker Deployment (Inference-Ready Container)

This project includes a Dockerfile that packages:

The inference script

All dependencies

Model loading

A default demo prediction

Build the image:
docker build -t anli-nli .

Run inference:
docker run --rm anli-nli

Example CMD in Dockerfile:
CMD ["python", "run_pipeline.py", "--mode", "demo",
     "--premise", "A man is playing music",
     "--hypothesis", "A man is playing guitar"]


Even if the interviewer doesnâ€™t run Docker, including it demonstrates deployment capability.
